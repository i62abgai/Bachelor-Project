{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RnJsC81ZGNab"
   },
   "source": [
    "# **Importing the libraries and define functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 361
    },
    "colab_type": "code",
    "id": "iZYc9mGxi1om",
    "outputId": "d4026d97-1199-4603-e4d6-2406091227a6"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "import itertools\n",
    "#Parameter estimation\n",
    "import optunity\n",
    "import optunity.metrics\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "#SVR model\n",
    "from sklearn.svm import SVR\n",
    "#Feature scaling (Normalize)\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#Noise reduction\n",
    "from scipy.signal import savgol_filter\n",
    "#Metrics\n",
    "from scipy import stats\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import accuracy_score\n",
    "#Visualize\n",
    "import matplotlib.pyplot as plt\n",
    "#File management\n",
    "import os.path\n",
    "import os, glob\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group of functions that reverse the standar scale, and normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_scaler(y_true, y_pred):\n",
    "    y_true = y_true.reshape(-1,1)\n",
    "    y_pred = y_pred.reshape(-1,1)\n",
    "    y_true = sc_y.inverse_transform(y_true)\n",
    "    y_pred = sc_y.inverse_transform(y_pred)\n",
    "    y_true = y_true.ravel()\n",
    "    y_pred = y_pred.ravel()\n",
    "    return y_true, y_pred\n",
    "\n",
    "def inverse_normalize(x):\n",
    "    for i, ob in enumerate(x):\n",
    "        x[i]=(x[i]*(dataset['CO2'].max()-dataset['CO2'].min())+dataset['CO2'].min())\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function allow us to get the percentage to split the data in order to get the number of instances equivalent to a period of time (1 hour, 3 hours...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_percentage(x, total):\n",
    "    return ((x*100)/total)/100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function that plots the predictions and the true values to compare them, and saves the results as csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accuracy(y_true, y_pred, xax_, step_, t_s):\n",
    "    y_true = inverse_normalize(y_true)\n",
    "    y_pred = inverse_normalize(y_pred)\n",
    "    y_true = y_true.reshape(-1,1)\n",
    "    y_pred = y_pred.reshape(-1,1)\n",
    "    xax=xax_\n",
    "    ig=plt.figure(figsize=(25, 12), dpi= 80, facecolor='w', edgecolor='k')\n",
    "    plt.plot(y_true[0::xax], label='y_true')\n",
    "    plt.plot(y_pred[0::xax], label='y_pred')\n",
    "    min_y_bound = min(min(y_true), min(y_true))\n",
    "    max_x_bound = max(max(y_true), max(y_true))\n",
    "    step = step_\n",
    "    plt.yticks(np.arange(min_y_bound, max_x_bound+step, step))\n",
    "    plt.xticks(np.arange(0, y_true[0::xax].size+1,1), np.arange(0, y_true.size+1,1)[0::xax], rotation=90)\n",
    "    plt.xlabel('Instances')\n",
    "    plt.ylabel('CO2')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.savefig(\"graphs/\"+filename+\"_{test_s:.4f}test_score{score:.3f}_g{g:.2f}_C{C:.2f}.png\".format(test_s=t_s, score = test_score, g = gamma, C=C))\n",
    "    fname = \"predictions/\"+filename+\"_{test_s:.4f}test_score{score:.3f}_g{g:.2f}_C{C:.2f}.csv\".format(test_s=t_s, score = test_score, g = gamma, C=C)\n",
    "    y_true = np.ravel(y_true)\n",
    "    y_pred = np.ravel(y_pred)\n",
    "    df = pd.DataFrame({\"y_true\" : y_true, \"y_pred\" : y_pred})\n",
    "    df.to_csv(fname, index=False, sep=\";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to do the parameter optimization we need to import the libraries and the metrics in which we are going to measure the quality of the model:\n",
    "\n",
    "\n",
    "*   R2_Score \\begin{equation} R2 = 1-\\frac{\\sum \\left (  y_{i}-\\check{y_{i}} \\right )^2}{\\sum \\left (  y_{i}-\\bar{y_{i}} \\right )^2} \\end{equation}\n",
    "*   Mean Squared Error (MSE) \\begin{equation} MSE = \\frac{1}{n} \\sum \\left ( y_{i}-\\check{y_{i}}\\right )^{2} \\end{equation}\n",
    "*   Mean Absolute Error (MAE) \\begin{equation} MAE = \\frac{1}{n} \\sum \\left |  y_{i}-\\check{y_{i}}\\right | \\end{equation}\n",
    "*   Root Mean Squared Error (RMSE) \\begin{equation} RMSE = \\sqrt{MSE} = \\sqrt{\\frac{1}{n} \\sum \\left ( y_{i}-\\check{y_{i}}\\right )^{2}} \\end{equation}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_results(y_true, y_pred):\n",
    "    explained_variance=metrics.explained_variance_score(y_true, y_pred)\n",
    "    mean_absolute_error=metrics.mean_absolute_error(y_true, y_pred) \n",
    "    mse=metrics.mean_squared_error(y_true, y_pred) \n",
    "    r2=metrics.r2_score(y_true, y_pred)\n",
    "    metrics_dic = {'ev':round(explained_variance,4), 'r2':round(r2,4), 'mae':round(mean_absolute_error,4), 'mse':round(mse,4), 'rmse':round(np.sqrt(mse),4)}\n",
    "    return metrics_dic   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y0w1xHSgGarr"
   },
   "source": [
    "# **Importing the dataset**\n",
    "Read the dataset, and establish as index the date. This allows us to split data between different dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'livingroom_K4'\n",
    "dataset = pd.read_csv('data/'+filename+'.csv', delimiter=\";\", parse_dates=True)\n",
    "dataset['Time'] = pd.to_datetime(dataset.Time) \n",
    "dataset['Timestamp'] = pd.to_datetime(dataset.Time,format='%d-%m-%Y %H:%M') \n",
    "dataset.index = dataset.Timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data visualization**\n",
    "### Plot the data in order to visualize possible outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,1,figsize=(25,25))\n",
    "ax[0].scatter(dataset['CO2'], dataset['H'], s=3, cmap=\"Blues\")\n",
    "ax[0].set_xlabel('CO2 (PPM)')\n",
    "ax[0].set_ylabel('H (%)')\n",
    "ax[1].scatter(dataset['CO2'], dataset['T'], s=3, cmap=\"Blues\")\n",
    "ax[1].set_xlabel('CO2 (PPM)')\n",
    "ax[1].set_ylabel('T (°C)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, it's possible to see some outliers, so we have to work to detect and delete them so we get better predictions\n",
    "### Plot the data with and without applying the Savitzky-golay filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['CO2_']= savgol_filter(dataset['CO2'], 7, 1)\n",
    "dataset['H_']= savgol_filter(dataset['H'], 7, 1)\n",
    "dataset['T_']= savgol_filter(dataset['T'], 11, 1)\n",
    "fig, axs = plt.subplots(2,2,figsize=(30,25))\n",
    "\n",
    "start = '22-Dec-2018 12:00:00'\n",
    "end = '22-Dec-2018 18:00:00'\n",
    "\n",
    "axs[0][0].plot(dataset['CO2'][start:end])\n",
    "axs[0][0].plot(dataset['CO2_'][start:end])\n",
    "axs[0][0].legend(('Real', 'Filtered'), loc='upper right', shadow=True)\n",
    "axs[0][0].set_xlabel('Time (Hours)')\n",
    "axs[0][0].set_ylabel('CO2(PPM)')\n",
    "plt.sca(axs[0][0])\n",
    "plt.grid()\n",
    "\n",
    "axs[0][1].plot(dataset['H'][start:end])\n",
    "axs[0][1].plot(dataset['H_'][start:end])\n",
    "axs[0][1].legend(('Real', 'Filtered'), loc='upper right', shadow=True)\n",
    "axs[0][1].set_xlabel('Time (Hours)')\n",
    "axs[0][1].set_ylabel('H(%)')\n",
    "plt.sca(axs[0][1])\n",
    "plt.grid()\n",
    "\n",
    "axs[1][0].plot(dataset['T'][start:end])\n",
    "axs[1][0].plot(dataset['T_'][start:end])\n",
    "axs[1][0].legend(('Real', 'Filtered'), loc='upper right', shadow=True)\n",
    "axs[1][0].set_xlabel('Time (Hours)')\n",
    "axs[1][0].set_ylabel('T(°C)')\n",
    "plt.sca(axs[1][0])\n",
    "plt.grid()\n",
    "\n",
    "dataset=dataset.drop(['CO2_', 'H_', 'T_'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data preprocessing**\n",
    "\n",
    "### Detect and remove outliers\n",
    "Calculate Z-score to remove outliers which their Z-score is lower than 3.\n",
    "\\begin{equation}\n",
    "    z = \\frac{x-\\mu}{\\sigma}\n",
    "\\end{equation}\n",
    "Print the shape to see how many instances are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_shape = dataset.shape\n",
    "print(original_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.abs(stats.zscore(dataset.iloc[:,1:4]))\n",
    "threshold = 3\n",
    "dataset = dataset[(z < threshold).all(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "removed_outliers = dataset.shape\n",
    "print(original_shape[0]-removed_outliers[0], \"instances were detected as outliers and removed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data splitting\n",
    "We split all the dataset into train, validation and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.iloc[:,2:4].values.astype(float)\n",
    "y = dataset.iloc[:,1:2].values.astype(float)\n",
    "print(\"Shapes train (X,y): \\t\", X.shape,y.shape, \"\\nData \\nX:\", X, \"\\ny:\", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check linear data\n",
    "Check if data is linear: Use LinearRegression, if R2 score is lower than 0, data is nonlinear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "regressor = LinearRegression().fit(X, y)\n",
    "y_pred, y_true = regressor.predict(X), y\n",
    "regression_results(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data filtering (Noise reduction)\n",
    "Filter data with Savitzky-golay filter. The windows length is set to 7, and the polynomial order to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['CO2']= savgol_filter(dataset['CO2'], 7, 1)\n",
    "dataset['H']= savgol_filter(dataset['H'], 7, 1)\n",
    "dataset['T']= savgol_filter(dataset['T'], 7, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data normalization\n",
    "In order to normalize the data we apply to the dataset the next formula:\n",
    "\\begin{equation}\n",
    "    X_{norm} = \\frac{X-X_{min}}{X_{max}-X_{min}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gpjuuvxPi1o0"
   },
   "outputs": [],
   "source": [
    "dataset_normalized = ((dataset-dataset.min())/(dataset.max()-dataset.min()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable preparation\n",
    "Get the percentages of the different test sizes (3 hour, 6 hour, 1 day, 3 day), to test the trained model in different ranges of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hour = get_percentage(60, dataset.shape[0])\n",
    "three_hour = get_percentage(180, dataset.shape[0])\n",
    "six_hour = get_percentage(360, dataset.shape[0])\n",
    "one_day = get_percentage(1440, dataset.shape[0])\n",
    "three_day = get_percentage(3*1440, dataset.shape[0])\n",
    "\n",
    "test_s = [one_hour, three_hour, six_hour, one_day, three_day]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the dataframe which we will export later, to save the scores and parameters of each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.DataFrame(columns=['ev', 'r2', 'mae', 'mse', 'rmse', 'gamma', 'C', 'test_size'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wjii537zIMu6"
   },
   "source": [
    "### Data and class\n",
    "Now prepare the X and y variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_XY(dataset_normalized):\n",
    "    X = dataset_normalized.iloc[:,2:4].values.astype(float)\n",
    "    y = dataset_normalized.iloc[:,1:2].values.astype(float)\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data scalation\n",
    "Scale the data in order to get better accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_X = StandardScaler()\n",
    "sc_y = StandardScaler()\n",
    "\n",
    "def scale_data(X,y):\n",
    "    X = sc_X.fit_transform(X)\n",
    "    y = sc_y.fit_transform(y)\n",
    "    y = np.ravel(y)\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data between train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(X, y):\n",
    "    return train_test_split(X, y, train_size=0.7, test_size=test_s[i], random_state=55)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter estimation\n",
    "\n",
    "GridSearchCV is a tool offered by Scikit-learn which help us to set different parameters and it will select and train the model with the best combination of those parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_parameters(X_train, y_train):\n",
    "    gamma_range = [100, 150]\n",
    "    C_range = [3]\n",
    "    tuned_parameters = [{'kernel': ['rbf'], 'gamma': gamma_range, 'C': C_range }]\n",
    "    grid_regressor = GridSearchCV(SVR(), tuned_parameters, scoring='r2', cv=3)\n",
    "    grid_regressor.fit(X_train, y_train)\n",
    "    return grid_regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model train and creation\n",
    "Create the model with the optimal parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(C, gamma, X_train, y_train):\n",
    "    regressor = SVR(kernel='rbf',\n",
    "                    C=C,\n",
    "                    gamma=gamma,\n",
    "                    epsilon=0.125)\n",
    "    regressor.fit(X_train,y_train)\n",
    "    return regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_data(regressor, X_, y_):\n",
    "    y_true, y_pred = y_, regressor.predict(X_)\n",
    "    return y_true, y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execute the code\n",
    "Execute all the functions created before in order to obtain the model and the scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,o in enumerate(test_s):\n",
    "\n",
    "    #Split between traind and test\n",
    "    print('Data split (Traind and test)')\n",
    "    print('\\tTest instances: ', round(dataset.shape[0]*o,0))\n",
    "    print('\\tTrain instances: ', round(dataset.shape[0]*0.7),0)\n",
    "\n",
    "    #Split between X and y\n",
    "    X, y = split_XY(dataset_normalized)\n",
    "\n",
    "    #Data normalize\n",
    "    print('Data scale')\n",
    "    X,y = scale_data(X,y)\n",
    "    \n",
    "    X_train, X_, y_train, y_ = split_data(X,y)\n",
    "\n",
    "    #Parameter estimation\n",
    "    print('Parameter tuning')\n",
    "    grid_regressor=estimate_parameters(X_train, y_train)\n",
    "\n",
    "    #Save best parameters\n",
    "    C = grid_regressor.best_params_['C']\n",
    "    gamma = grid_regressor.best_params_['gamma']\n",
    "    print('Training model...')\n",
    "\n",
    "    #Model creation\n",
    "    regressor=create_model(C, gamma, X_train, y_train)\n",
    "\n",
    "    #Predict test\n",
    "    print('Predicting...')\n",
    "    y_true, y_pred = predict_data(regressor, X_, y_)\n",
    "\n",
    "    #Inverse the data scale\n",
    "    y_true,y_pred = inverse_scaler(y_true, y_pred)\n",
    "\n",
    "    print('Saving results...\\n')\n",
    "    #Get scores and save them\n",
    "    results_test=regression_results(y_true,y_pred)\n",
    "    test_score = results_test['r2']\n",
    "    results_test['gamma']=gamma\n",
    "    results_test['C']=C\n",
    "    results_test['test_size']=round(test_s[i],4)\n",
    "    test_dic = pd.DataFrame(results_test, index=[0])\n",
    "    output = output.append(test_dic, ignore_index=True)\n",
    "\n",
    "    #Plot the graphs\n",
    "    plot_accuracy(y_true, y_pred, int(y_true.size/50), 100, test_s[i])\n",
    "\n",
    "print('Done')\n",
    "print(output)\n",
    "output.to_csv('configuration_models/models_.csv', mode='a', header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manually remove the memory occupied with the garbage collector to not freeze the computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}